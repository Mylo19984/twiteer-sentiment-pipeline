{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6580c8a-3491-4c68-837f-cdc71e2b02cf",
   "metadata": {},
   "source": [
    "### Importing pyspark library adn other necessary for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b083dec6-67b2-45a1-9cfe-4380565e1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, BooleanType, LongType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, TimestampType, ArrayType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "304b4bc4-eaa5-4e75-9d58-d33d00282a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "060ff934-efac-41ee-88ac-79be7bfc9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e467f3c-7366-497b-8573-d82bf96839c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04b25497-3728-4808-9403-339e6f7ffee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63abaa9b-c366-467c-9833-3c6f21e0ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb32cfc5-c34e-4c2f-a9ef-9fd9bd256b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.24.82)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.82 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.27.82)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.28.0,>=1.27.82->boto3) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.28.0,>=1.27.82->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.82->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c6f6eaa-f035-42a2-8fd4-114d2936b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import configparser\n",
    "import json\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab124aa0-8a37-48b0-806b-441f956d598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj = configparser.ConfigParser()\n",
    "config_obj.read('config.ini')\n",
    "# db_param = config_obj[\"postgresql\"]\n",
    "aws_user = config_obj[\"aws\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad24701b-3943-4dce-9ef6-952045049abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\n",
    "     service_name='s3',\n",
    "     region_name=aws_user['region'],\n",
    "     aws_access_key_id=aws_user['acc_key'],\n",
    "     aws_secret_access_key=aws_user['secret_acc_key']\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c25ab48-c687-4f67-a15d-8b95e4bffec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3.get_object(Bucket='mylosh', Key=F'tweet/elon.json')\n",
    "j = json.loads(obj['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a012c37-ce44-4d6f-bec6-f80200649dba",
   "metadata": {},
   "source": [
    "#### Creating a session, structure type and data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2be7190-d41c-4df0-9fca-fe626ec62925",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName('mylo')\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7abb8ed9-a967-445b-91a1-e55e8b35d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_tweet = StructType([\n",
    "StructField(\"author_id\", IntegerType(),True),\n",
    "StructField(\"username\", StringType(),True),\n",
    "StructField(\"author_followers\", IntegerType(),True),\n",
    "StructField(\"author_tweets\", IntegerType(),True),\n",
    "StructField(\"author_description\", StringType(),True),\n",
    "StructField(\"author_location\", StringType(),True),\n",
    "StructField(\"tweet_id\", LongType(),True),\n",
    "StructField(\"text\", StringType(),True),\n",
    "StructField(\"created_at\", StringType(),True),\n",
    "StructField(\"retweets\", IntegerType(),True),\n",
    "StructField(\"replies\", IntegerType(),True),\n",
    "StructField(\"likes\", IntegerType(),True),\n",
    "StructField(\"quote_count\", IntegerType(),True),\n",
    "StructField(\"conversation_id\", LongType(),True),\n",
    "StructField(\"attach_list\", \n",
    "            StructType(\n",
    "            [\n",
    "                StructField(\"media_keys\", StringType(), True)\n",
    "            ]\n",
    "                        )\n",
    "            ,True),\n",
    "                StructField(\"referenced_tweets_list\", \n",
    "            StructType([\n",
    "            StructField(\"data\",\n",
    "            StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"id\", StringType(), True)\n",
    "                ])\n",
    "            , True)\n",
    "            ])\n",
    "            ,True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1da818a9-b70e-457a-9ed8-0ce84e70fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(j, schema=schema_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f4128e5-73e1-4124-8e26-d70e38c1074c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('author_id', IntegerType(), True), StructField('username', StringType(), True), StructField('author_followers', IntegerType(), True), StructField('author_tweets', IntegerType(), True), StructField('author_description', StringType(), True), StructField('author_location', StringType(), True), StructField('tweet_id', LongType(), True), StructField('text', StringType(), True), StructField('created_at', StringType(), True), StructField('retweets', IntegerType(), True), StructField('replies', IntegerType(), True), StructField('likes', IntegerType(), True), StructField('quote_count', IntegerType(), True), StructField('conversation_id', LongType(), True), StructField('attach_list', StructType([StructField('media_keys', StringType(), True)]), True), StructField('referenced_tweets_list', StructType([StructField('data', StructType([StructField('type', StringType(), True), StructField('id', StringType(), True)]), True)]), True)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cfc73-0b1a-446a-b5de-93df17e1b2fb",
   "metadata": {},
   "source": [
    "#### Removing garbage from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9d79882-386f-4ab2-a9ee-f702b2587762",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~@'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fcb8fa0-00fb-4d31-995b-783b12939fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tweet):\n",
    "    t = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf26ad9b-dd6e-431e-b5d7-cbfa32980cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b28c645-2020-4281-ad01-29dd12242cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5096a9cc-b390-4127-9d30-040d106feaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab00268a-6d49-42ed-abbe-37e706ff4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_links=udf(remove_links, StringType())\n",
    "remove_punctuation=udf(remove_punctuation, StringType())\n",
    "remove_users = udf(remove_users, StringType())\n",
    "remove_hashtag = udf(remove_hashtag, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a9a7cc6-650c-4359-8952-2bd0848149b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_text = df.withColumn('cleaned_tweet_text', remove_links(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f8932c3-5fed-435e-91d5-13cbc686d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_text = df_clean_text.withColumn('cleaned_tweet_text', remove_users(df_clean_text['cleaned_tweet_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ab05189-5bc1-442f-b6a4-0a99cd15baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_text = df_clean_text.withColumn('cleaned_tweet_text', remove_punctuation(df_clean_text['cleaned_tweet_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5b43f91-0cd7-472b-a126-8ae5e3b25899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_text = df_clean_text.withColumn('cleaned_tweet_text', remove_hashtag(df_clean_text['cleaned_tweet_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40823237-7585-48a6-8763-8fa60b34d472",
   "metadata": {},
   "source": [
    "#### Inserting sentiment analysis in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7288b3a6-3b5d-42b7-acc6-affb9afb355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c080c05-4c0d-4747-912c-6a850da07a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7626f8e6-77de-4284-83de-3afc2a6ad30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "622702dc-9e13-4a37-99dd-7e7609383256",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a878aed7-0f42-4917-ae2b-6b2cc77a268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model - cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "857658ad-8130-4c53-8636-1e6dd78cef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_berta = 'cardiffnlp/twitter-roberta-base-sentiment-latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ba7ea6c-3cad-4343-8f5e-076538d2f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_berta = AutoModelForSequenceClassification.from_pretrained(model_name_berta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8468fac6-ce48-4704-9ab0-78fa17c44261",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_berta = AutoTokenizer.from_pretrained(model_name_berta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5240fa0-6e55-40ba-b145-dec4c5fce060",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_berta = pipeline('sentiment-analysis', model=model_berta, tokenizer=tokenizer_berta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8afdc79-dd50-4a0e-b62d-408ecdaac6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def berta_classifier_sentiment_lable(text):\n",
    "    text = classifier_berta(text)\n",
    "    return text[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8abdc639-a67a-426b-b580-807c5cb8c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def berta_classifier_sentiment_score(text):\n",
    "    text = classifier_berta(text)\n",
    "    return text[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "31d23eb1-9bd2-4f72-bd95-e13448896771",
   "metadata": {},
   "outputs": [],
   "source": [
    "berta_classifier_sentiment_lable = udf(berta_classifier_sentiment_lable, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2c42b1b-81f9-4af3-a5ac-f6a952878975",
   "metadata": {},
   "outputs": [],
   "source": [
    "berta_classifier_sentiment_score = udf(berta_classifier_sentiment_score, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1aeaf16b-3603-48c4-9a66-a705392bb8d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o13.broadcast.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:316)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:316)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1710/0x0000000801705bb8.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n\tat org.apache.spark.util.Utils$.$anonfun$copyStream$1(Utils.scala:373)\n\tat org.apache.spark.util.Utils$$$Lambda$1066/0x0000000801257250.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.util.Utils$.copyStream(Utils.scala:380)\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$writeObject$1(PythonRDD.scala:757)\n\tat org.apache.spark.api.python.PythonBroadcast$$Lambda$1715/0x000000080170f1d8.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n\tat org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:753)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1070)\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1516)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_sentiment_label \u001b[38;5;241m=\u001b[39m df_clean_text\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mberta_sent_analysis_label\u001b[39m\u001b[38;5;124m'\u001b[39m, berta_classifier_sentiment_lable(df_clean_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_tweet_text\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:276\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:249\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    247\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_judf(func)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_judf\u001b[49m\n\u001b[1;32m    251\u001b[0m jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:215\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_judf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:224\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    221\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_getActiveSessionOrCreate()\n\u001b[1;32m    222\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[0;32m--> 224\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py:50\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_function\u001b[39m(\n\u001b[1;32m     47\u001b[0m     sc: SparkContext, func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any], returnType: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataTypeOrString\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[1;32m     49\u001b[0m     command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[0;32m---> 50\u001b[0m     pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m     54\u001b[0m         env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3349\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3346\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m     \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[0;32m-> 3349\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickled_command\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3350\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(broadcast)\n\u001b[1;32m   3351\u001b[0m broadcast_vars \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39m_jbroadcast \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_pickled_broadcast_vars]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:1193\u001b[0m, in \u001b[0;36mSparkContext.broadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcast[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m    Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    object for reading it in distributed functions. The variable will\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    be sent to each cluster only once.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pickled_broadcast_vars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/broadcast.py:134\u001b[0m, in \u001b[0;36mBroadcast.__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_encryption_enabled:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_broadcast\u001b[38;5;241m.\u001b[39mwaitTillDataReceived()\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jbroadcast \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_broadcast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickle_registry \u001b[38;5;241m=\u001b[39m pickle_registry\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# we're on an executor\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o13.broadcast.\n: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1(TorrentBroadcast.scala:316)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$1$adapted(TorrentBroadcast.scala:316)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$$Lambda$1710/0x0000000801705bb8.apply(Unknown Source)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\n\tat org.apache.spark.util.Utils$.$anonfun$copyStream$1(Utils.scala:373)\n\tat org.apache.spark.util.Utils$$$Lambda$1066/0x0000000801257250.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.util.Utils$.copyStream(Utils.scala:380)\n\tat org.apache.spark.api.python.PythonBroadcast.$anonfun$writeObject$1(PythonRDD.scala:757)\n\tat org.apache.spark.api.python.PythonBroadcast$$Lambda$1715/0x000000080170f1d8.apply$mcJ$sp(Unknown Source)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1470)\n\tat org.apache.spark.api.python.PythonBroadcast.writeObject(PythonRDD.scala:753)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat java.base/java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1070)\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1516)\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:321)\n"
     ]
    }
   ],
   "source": [
    "df_sentiment_label = df_clean_text.withColumn('berta_sent_analysis_label', berta_classifier_sentiment_lable(df_clean_text['cleaned_tweet_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c6d2a-feb4-4219-bfa4-8a972372de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = df_new.withColumn('berta_sent_analysis_score', berta_classifier_sentiment_score(df_new['cleaned_tweet_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3a9b2-a06b-424b-8353-030270752599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_label.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ab595-3f2d-42ae-ac5c-5d02e75dbea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_label.select(df_sentiment_label['tweet_id'], df_sentiment_label['cleaned_tweet_text'], df_sentiment_label['berta_sent_analysis_label']).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb4de9-efae-4e82-b0a1-f2470b9a57a9",
   "metadata": {},
   "source": [
    "#### Transformation of the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4357b5-065c-4d96-8742-9f5b542aec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_manipul = df_sentiment_label.withColumn('created_at', (df_sentiment_label['created_at']/1000).cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ec9c5-1df3-4046-a9f3-9eae2dd6d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_manipul = df_date_manipul.withColumn('created_at_date', f.from_utc_timestamp(df_date_manipul['created_at'].cast(TimestampType()), 'PST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06942f-c530-41fe-86b5-42760668c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_manipul.select(df_date_manipul['tweet_id'], df_date_manipul['created_at'], df_date_manipul['created_at_date'], df_date_manipul['berta_sent_analysis_label']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bc631-c5b0-4685-9e57-decaaa33528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_date_manipul.drop('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b3235-b465-43c8-8cff-85fb6ac81de9",
   "metadata": {},
   "source": [
    "#### Final data frame for inserting in the mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f632a-c778-455b-8cd8-71dc904b399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
